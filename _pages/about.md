---
layout: about
title: About
permalink: /
subtitle:  Second-year ECE Ph.D. Student@University of Washington, yadongli@uw.edu

profile:
  align: left
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
      <a href="https://scholar.google.com/citations?user=iZWM3pAAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-2x"></i></a>
      <a href="https://github.com/leeyadong"><i class="fa-brands fa-square-github fa-2x"></i></a>
      <a href="https://www.linkedin.com/in/yadong-li-737b4032a/"><i class="fa-brands fa-linkedin fa-2x"></i></a>
    


news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

I am a 2nd year Ph.D. student in the Department of Electrical & Computer Engineering at the **University of Washington (UW)**, advised by [Prof. Akshay Gadre](https://www.akshaygadre.com/). Previously, my research journey began at the **University of Science and Technology of China (USTC)**, working with [Prof. Qibin Sun](https://scholar.google.com/citations?hl=en&user=bPLvsSAAAAAJ) and [Prof. Yan Chen](https://scholar.google.com/citations?hl=en-EN&user=MVOCn1AAAAAJ).

My research aims to bring ambient intelligence to everyday wireless and wearable devices through cutting-edge AI, multimodal sensing, and physics-driven modeling.   
  - **mmWave and NLoS Imaging:** developing compact, mobile imaging systems that achieve 3D, high-resolution non-line-of-sight imaging via low-cost mmWave platforms.: [IFNet](https://ieeexplore.ieee.org/abstract/document/10740682) (TMC'24), [PSF-SAR](https://www.nature.com/articles/s44172-023-00156-2) (CommsEng'24), [mmHide](https://ieeexplore.ieee.org/document/11007126) (TIFS'25), [DREAM-PCD](https://ieeexplore.ieee.org/abstract/document/10794585) (TIP'24).
    
  - **Wireless and Wearable Sensing:** designing intelligent, human-centric sensing systems that leverage wireless and wearable sensors for health monitoring, motion tracking, and interactive applications: [UltraPoser]() (UIST'25), [RF-Mamba](https://openreview.net/forum?id=lG9fjBLb6d) (ICLR'25), [RF-HRV](https://www.nature.com/articles/s41467-024-55061-9) (NatureComms'24), [RLoc](https://dl.acm.org/doi/abs/10.1145/3631437) (UbiComp'24).

Motivated by the lack of benchmarks in wireless AI, my co-authors and I have released datasets and code for diverse signals and applications, which have been accessed by researchers from **over 100 institutions across 12 countries**:  
- [IFNet](https://github.com/leeyadong/IFNet): Handheld mmWave SAR imaging dataset (10400 samples from 26 objects and 200 handheld scanning trajectories).
- [MCD-Gesture](https://github.com/leeyadong/cross_domain_gesture_dataset): Cross-domain mmWave gesture dataset (24050 instances from 25 users, 6 rooms, and 5 locations). 
- [H-WILD](https://github.com/H-WILD/human_held_device_wifi_indoor_localization_dataset): Human-held device WiFi localization dataset (120,000 frames from 10 volunteers across 4 rooms).
- [RadarEyes](https://github.com/ruixv/RadarEyes): Multi-modal point cloud dataset (1,000,000 frames of radar and LiDAR data from 300 rooms).

