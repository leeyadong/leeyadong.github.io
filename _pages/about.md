---
layout: about
title: About
permalink: /
subtitle:  First-year ECE Ph.D. Student@University of Washington, yadongli@uw.edu

profile:
  align: left
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
      <a href="https://scholar.google.com/citations?user=iZWM3pAAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-2x"></i></a>
      <a href="https://github.com/leeyadong"><i class="fa-brands fa-square-github fa-2x"></i></a>
      <a href="https://www.linkedin.com/in/yadong-li-737b4032a/"><i class="fa-brands fa-linkedin fa-2x"></i></a>
    


news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

I am a 1st year Ph.D. student in the Department of Electrical & Computer Engineering at the **University of Washington (UW)**. As a member of the [Networking and Emerging Wireless Technologies (NEWT) Lab](https://newtlab.ece.uw.edu/) and under the guidance of [Prof. Akshay Gadre](https://www.akshaygadre.com/), I am particularly interested in developing **wireless technologies for autonomous systems**, aimed at creating autonomous robots that operate reliably in challenging environments, as well as empowering wireless sensing with greater mobility and coverage. 
<!-- am particularly interested in developing **intelligent wireless technologies for robotics**, aimed at creating autonomous robots that operate reliably in challenging environments, as well as empowering wireless sensing with greater mobility and coverage.-->

Previously, my research journey began at the [Intelligent Perception (IP) Lab](https://ustc-ip-lab.github.io/) at the **University of Science and Technology of China (USTC)**. Under the guidance of [Prof. Qibin Sun](https://scholar.google.com/citations?hl=en&user=bPLvsSAAAAAJ) and [Prof. Yan Chen](https://scholar.google.com/citations?hl=en-EN&user=MVOCn1AAAAAJ), we focused on developing **human-centric wireless sensing** systems. Intending to enhance the sensing resolution and robustness, our work fosters innovation across diverse applicationsï¼š

- **Computational Imaging**: Phase compensation for handheld SAR imaging with physical models ([Comms Eng'24](https://www.nature.com/articles/s44172-023-00156-2)) and machine learning  ([ICASSP'24](https://ieeexplore.ieee.org/document/10447461)).
- **Motion Sensing**: Domain-independent human gesture sensing ([TMC'22](https://ieeexplore.ieee.org/abstract/document/9894724)), localization ([UbiComp'24](https://dl.acm.org/doi/abs/10.1145/3631437)), and tracking ([IJCAI'24](https://www.ijcai.org/proceedings/2024/674)).  
- **Mobile Health**: Motion-robust respiration sensing ([TMC'24](https://ieeexplore.ieee.org/document/10379134)) and long-term cardiac monitoring ([arXiv'24](https://www.researchsquare.com/article/rs-4456442/v1)).  


Motivated by the lack of datasets in wireless AI, we have also released several **large-scale benchmarks** for a variety of signals and applications, including:  
- [MCD-Gesture](https://github.com/leeyadong/cross_domain_gesture_dataset): Cross-domain mmWave gesture dataset (24050 instances from 25 users, 6 rooms, and 5 locations). 
- [H-WILD](https://github.com/H-WILD/human_held_device_wifi_indoor_localization_dataset): Human-held device WiFi localization dataset (120,000 frames from 10 volunteers across 4 rooms).
- [RadarEyes](https://github.com/ruixv/RadarEyes): Multi-modal point cloud dataset (1,000,000 frames of radar and LiDAR data from 300 rooms).
- [RF-UNIT](): RF-based unconstrained indoor human tracking dataset (4,000,000 radar heatmaps from 6 rooms and 19 people).
  
