<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>I am an incoming Ph.D. student in the Department of Electrical &amp; Computer Engineering at the <strong>University of Washington</strong>. I am a group member of the <a href="https://newtlab.ece.uw.edu/" rel="external nofollow noopener" target="_blank">Networking and Emerging Wireless Technologies (NEWT) Lab</a>, advised by <a href="https://www.akshaygadre.com/" rel="external nofollow noopener" target="_blank">Prof. Akshay Gadre</a>. The core focus of my Ph.D. research will be the development of <strong>wireless technologies for robotics</strong>, aimed at creating intelligent robots that operate reliably in challenging environments and enhancing wireless sensing with greater mobility and coverage.</p> <p>Previously, my research journey began at the <a href="https://ustc-ip-lab.github.io/" rel="external nofollow noopener" target="_blank">Intelligent Perception (IP) Lab</a> of the <strong>University of Science and Technology of China</strong>. Under the guidance of <a href="https://scholar.google.com/citations?hl=en&amp;user=bPLvsSAAAAAJ" rel="external nofollow noopener" target="_blank">Prof. Qibin Sun</a> and <a href="https://scholar.google.com/citations?hl=en-EN&amp;user=MVOCn1AAAAAJ" rel="external nofollow noopener" target="_blank">Prof. Yan Chen</a>, we focused on developing <strong>human-centric wireless sensing</strong> systems. Intending to enhance the sensing resolution and robustness, our work contributes innovation across a range of applications：</p> <ul> <li>Domain-independent human gesture sensing (<a href="https://ieeexplore.ieee.org/abstract/document/9894724" rel="external nofollow noopener" target="_blank">TMC’22</a>), localization (<a href="https://dl.acm.org/doi/abs/10.1145/3631437" rel="external nofollow noopener" target="_blank">UbiComp’24</a>), and tracking (<a href="https://www.ijcai.org/proceedings/2024/674" rel="external nofollow noopener" target="_blank">IJCAI’24</a>).</li> <li>Motion-robust respiration sensing (<a href="https://ieeexplore.ieee.org/document/10379134" rel="external nofollow noopener" target="_blank">TMC’24</a>) and long-term caidic monitoring (<a href="https://www.researchsquare.com/article/rs-4456442/v1" rel="external nofollow noopener" target="_blank">arXiv’24</a>).</li> <li>Super-resolution and handheld SAR imaging with physical models (<a href="https://www.nature.com/articles/s44172-023-00156-2" rel="external nofollow noopener" target="_blank">Comms Eng’24</a>) and machine learning (<a href="https://ieeexplore.ieee.org/document/10447461" rel="external nofollow noopener" target="_blank">ICASSP’24</a>).</li> </ul> <p>Motivated by the scarcity of datasets in the wireless domain, we have built various large-scale benchmarks for diverse signals and applications, including:</p> <ul> <li> <a href="https://github.com/leeyadong/cross_domain_gesture_dataset" rel="external nofollow noopener" target="_blank">MCD-Gesture</a>: Cross-domain mmWave gesture dataset consisting of 24050 instances from 25 users, 6 rooms, and 5 locations.</li> <li> <a href="https://github.com/H-WILD/human_held_device_wifi_indoor_localization_dataset" rel="external nofollow noopener" target="_blank">H-WILD</a>: Human-held device WiFi localization dataset consisting of 120,000 frames from 10 volunteers across 4 rooms.</li> <li> <a href="https://github.com/ruixv/RadarEyes" rel="external nofollow noopener" target="_blank">RadarEyes</a>: Multi-modal point cloud dataset consisting of 1 million frames of radar and LiDAR data from 300 rooms.</li> <li> <a href="">RF-UNIT</a>: RF-based unconstrained indoor human tracking dataset consisting of 4 million radar heatmaps from 6 rooms and 19 people.</li> </ul> </body></html>